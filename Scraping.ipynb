{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbLARzOqft-2",
        "colab_type": "text"
      },
      "source": [
        "### **Scraping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G25rkbKcfrcQ",
        "colab_type": "text"
      },
      "source": [
        "This is not my scraper. I utilized code from https://holwech.github.io/blog/Automatic-news-scraper/ since I wanted to add some more recent articles to my dataset. However, I modified some of the code so that I could scrape more articles than the scraper was originally scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6MRVrxXFxzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt3beinUHPOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diQAzNIjGxqN",
        "colab_type": "code",
        "outputId": "612ee26b-e8a6-4bdf-fc77-67b02d1dc222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "DRIVE_MOUNT='/content/gdrive'\n",
        "drive.mount(DRIVE_MOUNT)\n",
        "\n",
        "# create folder to write data to\n",
        "CIS545_FOLDER=os.path.join(DRIVE_MOUNT, 'My Drive', 'CIS545_2020')\n",
        "HOMEWORK_FOLDER=os.path.join(CIS545_FOLDER, 'Project')\n",
        "os.makedirs(HOMEWORK_FOLDER, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIpGURz6ZkHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cPCU4uvJWYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = {\n",
        "  \"cnn\": {\n",
        "    \"link\": \"http://edition.cnn.com/\"\n",
        "  },\n",
        "  \"bbc\": {\n",
        "    \"rss\": \"http://feeds.bbci.co.uk/news/rss.xml\",\n",
        "    \"link\": \"http://www.bbc.com/\"\n",
        "  },\n",
        "  \"theguardian\": {\n",
        "    \"rss\": \"https://www.theguardian.com/uk/rss\",\n",
        "    \"link\": \"https://www.theguardian.com/international\"\n",
        "  },\n",
        "  \"breitbart\": {\n",
        "    \"link\": \"http://www.breitbart.com/\"\n",
        "  },\n",
        "  \"infowars\": {\n",
        "    \"link\": \"https://www.infowars.com/\"\n",
        "  },\n",
        "  \"foxnews\": {\n",
        "    \"link\": \"http://www.foxnews.com/\"\n",
        "  },\n",
        "  \"nbcnews\": {\n",
        "    \"link\": \"http://www.nbcnews.com/\"\n",
        "  },\n",
        "  \"washingtonpost\": {\n",
        "    \"rss\": \"http://feeds.washingtonpost.com/rss/world\",\n",
        "    \"link\": \"https://www.washingtonpost.com/\"\n",
        "  },\n",
        "  \"theonion\": {\n",
        "    \"link\": \"http://www.theonion.com/\"\n",
        "  }\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxJaxXKwZBmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "json_object = json.dumps(dictionary, indent = 4) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1l1mxQdJWuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"NewsPapers.json\", \"w\") as outfile: \n",
        "    outfile.write(json_object) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcruXn0JEGv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import feedparser as fp\n",
        "import json\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "from time import mktime\n",
        "from datetime import datetime\n",
        "\n",
        "# Set the limit for number of articles to download\n",
        "LIMIT = 14500\n",
        "\n",
        "data = {}\n",
        "data['newspapers'] = {}\n",
        "\n",
        "# Loads the JSON files with news sites\n",
        "with open('NewsPapers.json') as data_file:\n",
        "    companies = json.load(data_file)\n",
        "\n",
        "count = 1\n",
        "\n",
        "# Iterate through each news company\n",
        "for company, value in companies.items():\n",
        "    # If a RSS link is provided in the JSON file, this will be the first choice.\n",
        "    # Reason for this is that, RSS feeds often give more consistent and correct data.\n",
        "    # If you do not want to scrape from the RSS-feed, just leave the RSS attr empty in the JSON file.\n",
        "    if 'rss' in value:\n",
        "        d = fp.parse(value['rss'])\n",
        "        print(\"Downloading articles from \", company)\n",
        "        newsPaper = {\n",
        "            \"rss\": value['rss'],\n",
        "            \"link\": value['link'],\n",
        "            \"articles\": []\n",
        "        }\n",
        "        for entry in d.entries:\n",
        "            # Check if publish date is provided, if no the article is skipped.\n",
        "            # This is done to keep consistency in the data and to keep the script from crashing.\n",
        "            if hasattr(entry, 'published'):\n",
        "                if count > LIMIT:\n",
        "                    break\n",
        "                article = {}\n",
        "                article['link'] = entry.link\n",
        "                date = entry.published_parsed\n",
        "                article['published'] = datetime.fromtimestamp(mktime(date)).isoformat()\n",
        "                try:\n",
        "                    content = Article(entry.link)\n",
        "                    content.download()\n",
        "                    content.parse()\n",
        "                except Exception as e:\n",
        "                    # If the download for some reason fails (ex. 404) the script will continue downloading\n",
        "                    # the next article.\n",
        "                    print(e)\n",
        "                    print(\"continuing...\")\n",
        "                    continue\n",
        "                article['title'] = content.title\n",
        "                article['text'] = content.text\n",
        "                newsPaper['articles'].append(article)\n",
        "                print(count, \"articles downloaded from\", company, \", url: \", entry.link)\n",
        "                count = count + 1\n",
        "    else:\n",
        "        # This is the fallback method if a RSS-feed link is not provided.\n",
        "        # It uses the python newspaper library to extract articles\n",
        "        print(\"Building site for \", company)\n",
        "        paper = newspaper.build(value['link'], memoize_articles=False)\n",
        "        newsPaper = {\n",
        "            \"link\": value['link'],\n",
        "            \"articles\": []\n",
        "        }\n",
        "        noneTypeCount = 0\n",
        "        for content in paper.articles:\n",
        "            if count > LIMIT:\n",
        "                break\n",
        "            try:\n",
        "                content.download()\n",
        "                content.parse()\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                print(\"continuing...\")\n",
        "                continue\n",
        "            # Again, for consistency, if there is no found publish date the article will be skipped.\n",
        "            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n",
        "            if content.publish_date is None:\n",
        "                print(count, \" Article has date of type None...\")\n",
        "                noneTypeCount = noneTypeCount + 1\n",
        "                if noneTypeCount > 100:\n",
        "                    print(\"Too many noneType dates, aborting...\")\n",
        "                    noneTypeCount = 0\n",
        "                    break\n",
        "                count = count + 1\n",
        "                continue\n",
        "            article = {}\n",
        "            article['title'] = content.title\n",
        "            article['text'] = content.text\n",
        "            article['link'] = content.url\n",
        "            article['published'] = content.publish_date.isoformat()\n",
        "            newsPaper['articles'].append(article)\n",
        "            print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n",
        "            count = count + 1\n",
        "            noneTypeCount = 0\n",
        "    count = 1\n",
        "    data['newspapers'][company] = newsPaper\n",
        "\n",
        "# Finally it saves the articles as a JSON-file.\n",
        "try:\n",
        "    with open('scraped_articles.json', 'w') as outfile:\n",
        "        json.dump(data, outfile)\n",
        "except Exception as e: print(e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjbEIgFDH9y8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('scraped_articles.json') as json_data:\n",
        "    d = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwZSsyj_IqgS",
        "colab_type": "code",
        "outputId": "91ba5abd-a643-40b3-9532-7437cfb62282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for i, site in enumerate((list(d['newspapers']))):\n",
        "    print(i, site)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 cnn\n",
            "1 bbc\n",
            "2 theguardian\n",
            "3 breitbart\n",
            "4 infowars\n",
            "5 foxnews\n",
            "6 nbcnews\n",
            "7 washingtonpost\n",
            "8 theonion\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z870rTwYIt58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "for i, site in enumerate((list(d['newspapers']))):\n",
        "    articles = list(d['newspapers'][site]['articles'])\n",
        "    if i == 0:\n",
        "        df = pd.DataFrame.from_dict(articles)\n",
        "        df[\"site\"] = site\n",
        "    else:\n",
        "        new_df = pd.DataFrame.from_dict(articles)\n",
        "        new_df[\"site\"] = site\n",
        "        df = pd.concat([df, new_df], ignore_index = True)     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-iiY1uWIweF",
        "colab_type": "code",
        "outputId": "569559ff-89e6-484b-a5d5-b3aa11e16af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1844, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyXeL62_I2LY",
        "colab_type": "code",
        "outputId": "7d3a20c9-7834-46f1-b654-8c6533e2a039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>link</th>\n",
              "      <th>published</th>\n",
              "      <th>site</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Meghan and Harry dial into London court hearin...</td>\n",
              "      <td>London (CNN) The Duke and Duchess of Sussex di...</td>\n",
              "      <td>http://edition.cnn.com/2020/04/24/uk/harry-and...</td>\n",
              "      <td>2020-04-24T00:00:00</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coronavirus may force the UK to rethink its re...</td>\n",
              "      <td>London (CNN) Coronavirus travel restrictions h...</td>\n",
              "      <td>http://edition.cnn.com/2020/04/17/europe/migra...</td>\n",
              "      <td>2020-04-17T00:00:00</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>British politicians are concerned a 'virtual p...</td>\n",
              "      <td>London (CNN) The UK Parliament is expected on ...</td>\n",
              "      <td>http://edition.cnn.com/2020/04/16/uk/uk-parlia...</td>\n",
              "      <td>2020-04-16T00:00:00</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>UK's concern for Boris Johnson overrides politics</td>\n",
              "      <td>Julia Hobsbawm is a social philosopher and aut...</td>\n",
              "      <td>http://edition.cnn.com/2020/04/10/opinions/uks...</td>\n",
              "      <td>2020-04-10T00:00:00</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Notable US Spies Fast Facts</td>\n",
              "      <td>(CNN) Here is a look at some US citizens who h...</td>\n",
              "      <td>http://edition.cnn.com/2014/06/09/us/imprisone...</td>\n",
              "      <td>2014-06-09T00:00:00</td>\n",
              "      <td>cnn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1839</th>\n",
              "      <td>Ambush near Congo’s Virunga park kills 12 rang...</td>\n",
              "      <td>Get our Coronavirus Updates newsletter\\n\\nRece...</td>\n",
              "      <td>https://www.washingtonpost.com/world/ambush-ne...</td>\n",
              "      <td>2020-04-24T05:09:10</td>\n",
              "      <td>washingtonpost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1840</th>\n",
              "      <td>2nd French court orders Amazon to better prote...</td>\n",
              "      <td>The standoff has drawn global attention, as wo...</td>\n",
              "      <td>https://www.washingtonpost.com/world/europe/2n...</td>\n",
              "      <td>2020-04-24T16:58:21</td>\n",
              "      <td>washingtonpost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>No more today: UK’s new testing site closes fo...</td>\n",
              "      <td>Clicking on the link, aspiring applicants were...</td>\n",
              "      <td>https://www.washingtonpost.com/world/europe/no...</td>\n",
              "      <td>2020-04-24T16:43:20</td>\n",
              "      <td>washingtonpost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1842</th>\n",
              "      <td>Nations back UN plan to speed wide rollout of ...</td>\n",
              "      <td>“This is a landmark collaboration to accelerat...</td>\n",
              "      <td>https://www.washingtonpost.com/world/asia_paci...</td>\n",
              "      <td>2020-04-24T15:48:18</td>\n",
              "      <td>washingtonpost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>Swedish group: Imprisoned Saudi activist dies ...</td>\n",
              "      <td>Get our Coronavirus Updates newsletter\\n\\nRece...</td>\n",
              "      <td>https://www.washingtonpost.com/world/middle_ea...</td>\n",
              "      <td>2020-04-24T15:17:13</td>\n",
              "      <td>washingtonpost</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1844 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  title  ...            site\n",
              "0     Meghan and Harry dial into London court hearin...  ...             cnn\n",
              "1     Coronavirus may force the UK to rethink its re...  ...             cnn\n",
              "2     British politicians are concerned a 'virtual p...  ...             cnn\n",
              "3     UK's concern for Boris Johnson overrides politics  ...             cnn\n",
              "4                           Notable US Spies Fast Facts  ...             cnn\n",
              "...                                                 ...  ...             ...\n",
              "1839  Ambush near Congo’s Virunga park kills 12 rang...  ...  washingtonpost\n",
              "1840  2nd French court orders Amazon to better prote...  ...  washingtonpost\n",
              "1841  No more today: UK’s new testing site closes fo...  ...  washingtonpost\n",
              "1842  Nations back UN plan to speed wide rollout of ...  ...  washingtonpost\n",
              "1843  Swedish group: Imprisoned Saudi activist dies ...  ...  washingtonpost\n",
              "\n",
              "[1844 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFHT4EO0Oc4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp scraped_articles.json \"/content/gdrive/My Drive/CIS545_2020/Project/\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}